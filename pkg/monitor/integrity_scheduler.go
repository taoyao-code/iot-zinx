package monitor

import (
	"context"
	"sync"
	"time"

	"github.com/bujia-iot/iot-zinx/internal/infrastructure/logger"
	"github.com/sirupsen/logrus"
)

// IntegrityScheduler æ•°æ®å®Œæ•´æ€§æ£€æŸ¥è°ƒåº¦å™¨
type IntegrityScheduler struct {
	// æ£€æŸ¥é—´éš”
	checkInterval time.Duration

	// ä¸Šä¸‹æ–‡å’Œå–æ¶ˆå‡½æ•°
	ctx    context.Context
	cancel context.CancelFunc

	// ç­‰å¾…ç»„
	wg sync.WaitGroup

	// æ˜¯å¦æ­£åœ¨è¿è¡Œ
	running bool
	mutex   sync.Mutex

	// ä¾èµ–çš„ç®¡ç†å™¨
	tcpMonitor         *TCPMonitor
	sessionManager     *SessionManager
	deviceGroupManager *DeviceGroupManager
}

// NewIntegrityScheduler åˆ›å»ºæ•°æ®å®Œæ•´æ€§æ£€æŸ¥è°ƒåº¦å™¨
func NewIntegrityScheduler(interval time.Duration) *IntegrityScheduler {
	ctx, cancel := context.WithCancel(context.Background())

	return &IntegrityScheduler{
		checkInterval: interval,
		ctx:           ctx,
		cancel:        cancel,
		running:       false,
	}
}

// SetDependencies è®¾ç½®ä¾èµ–çš„ç®¡ç†å™¨
func (is *IntegrityScheduler) SetDependencies(
	tcpMonitor *TCPMonitor,
	sessionManager *SessionManager,
	deviceGroupManager *DeviceGroupManager,
) {
	is.mutex.Lock()
	defer is.mutex.Unlock()

	is.tcpMonitor = tcpMonitor
	is.sessionManager = sessionManager
	is.deviceGroupManager = deviceGroupManager

	logger.Info("IntegrityScheduler: ä¾èµ–ç®¡ç†å™¨å·²è®¾ç½®")
}

// Start å¯åŠ¨å®šæœŸæ£€æŸ¥
func (is *IntegrityScheduler) Start() error {
	is.mutex.Lock()
	defer is.mutex.Unlock()

	if is.running {
		return nil
	}

	if is.tcpMonitor == nil || is.sessionManager == nil || is.deviceGroupManager == nil {
		logger.Error("IntegrityScheduler: å¯åŠ¨å¤±è´¥ï¼Œä¾èµ–ç®¡ç†å™¨æœªå®Œå…¨è®¾ç½®")
		return nil
	}

	is.running = true
	is.wg.Add(1)

	go is.schedulerLoop()

	logger.WithField("interval", is.checkInterval.String()).Info("IntegrityScheduler: æ•°æ®å®Œæ•´æ€§æ£€æŸ¥è°ƒåº¦å™¨å·²å¯åŠ¨")
	return nil
}

// Stop åœæ­¢å®šæœŸæ£€æŸ¥
func (is *IntegrityScheduler) Stop() {
	is.mutex.Lock()
	defer is.mutex.Unlock()

	if !is.running {
		return
	}

	is.running = false
	is.cancel()
	is.wg.Wait()

	logger.Info("IntegrityScheduler: æ•°æ®å®Œæ•´æ€§æ£€æŸ¥è°ƒåº¦å™¨å·²åœæ­¢")
}

// schedulerLoop è°ƒåº¦å™¨ä¸»å¾ªç¯
func (is *IntegrityScheduler) schedulerLoop() {
	defer is.wg.Done()

	ticker := time.NewTicker(is.checkInterval)
	defer ticker.Stop()

	// å¯åŠ¨åç«‹å³æ‰§è¡Œä¸€æ¬¡æ£€æŸ¥
	is.performIntegrityCheck("startup")

	for {
		select {
		case <-is.ctx.Done():
			logger.Debug("IntegrityScheduler: è°ƒåº¦å™¨å¾ªç¯å·²åœæ­¢")
			return
		case <-ticker.C:
			is.performIntegrityCheck("scheduled")
		}
	}
}

// performIntegrityCheck æ‰§è¡Œå®Œæ•´æ€§æ£€æŸ¥
func (is *IntegrityScheduler) performIntegrityCheck(trigger string) {
	startTime := time.Now()

	logFields := logrus.Fields{
		"trigger":   trigger,
		"timestamp": startTime.Format("2006-01-02 15:04:05"),
	}

	logger.WithFields(logFields).Info("IntegrityScheduler: å¼€å§‹æ•°æ®å®Œæ•´æ€§æ£€æŸ¥")

	var totalIssues []string
	checkResults := make(map[string]int)

	// 1. TCPMonitor å®Œæ•´æ€§æ£€æŸ¥
	// æ³¨æ„ï¼šTCPMonitor é‡æ„åæš‚æ—¶ç§»é™¤äº† integrityCheckerï¼Œè¿™é‡Œè·³è¿‡æ£€æŸ¥
	if is.tcpMonitor != nil {
		// TODO: å®ç° TCPMonitor çš„å®Œæ•´æ€§æ£€æŸ¥æ–¹æ³•
		checkResults["tcpMonitor"] = 0
	}

	// 2. SessionManager å®Œæ•´æ€§æ£€æŸ¥
	if is.sessionManager != nil {
		issues := is.sessionManager.CheckSessionIntegrity("scheduled-session")
		checkResults["sessionManager"] = len(issues)
		totalIssues = append(totalIssues, issues...)
	}

	// 3. DeviceGroupManager å®Œæ•´æ€§æ£€æŸ¥
	if is.deviceGroupManager != nil {
		issues := is.deviceGroupManager.CheckGroupIntegrity("scheduled-group")
		checkResults["deviceGroupManager"] = len(issues)
		totalIssues = append(totalIssues, issues...)
	}

	// 4. æ¸…ç†åƒµå°¸è®¾å¤‡ç»„
	var cleanedZombieGroups int
	if is.deviceGroupManager != nil {
		cleanedZombieGroups = is.deviceGroupManager.CleanupZombieGroups("scheduled-cleanup")
		checkResults["cleanedZombieGroups"] = cleanedZombieGroups
	}

	elapsed := time.Since(startTime)

	// è®°å½•æ£€æŸ¥ç»“æœ
	resultFields := logFields
	resultFields["elapsedMs"] = elapsed.Milliseconds()
	resultFields["totalIssues"] = len(totalIssues)
	resultFields["checkResults"] = checkResults
	resultFields["cleanedZombieGroups"] = cleanedZombieGroups

	if len(totalIssues) > 0 {
		resultFields["issues"] = totalIssues
		logger.WithFields(resultFields).Error("IntegrityScheduler: æ•°æ®å®Œæ•´æ€§æ£€æŸ¥å‘ç°é—®é¢˜")

		// ğŸ”§ è§¦å‘å‘Šè­¦ï¼ˆå¯ä»¥åœ¨è¿™é‡Œé›†æˆå‘Šè­¦ç³»ç»Ÿï¼‰
		is.triggerAlert(totalIssues, checkResults)
	} else {
		logger.WithFields(resultFields).Info("IntegrityScheduler: æ•°æ®å®Œæ•´æ€§æ£€æŸ¥é€šè¿‡")
	}
}

// triggerAlert è§¦å‘å‘Šè­¦
func (is *IntegrityScheduler) triggerAlert(issues []string, checkResults map[string]int) {
	// ğŸ”§ è¿™é‡Œå¯ä»¥é›†æˆå…·ä½“çš„å‘Šè­¦ç³»ç»Ÿï¼Œå¦‚é‚®ä»¶ã€çŸ­ä¿¡ã€é’‰é’‰ç­‰
	logger.WithFields(logrus.Fields{
		"alertType":    "DATA_INTEGRITY_VIOLATION",
		"issueCount":   len(issues),
		"issues":       issues,
		"checkResults": checkResults,
		"severity":     "HIGH",
	}).Error("IntegrityScheduler: æ•°æ®å®Œæ•´æ€§å‘Šè­¦è§¦å‘")

	// ç¤ºä¾‹ï¼šå¯ä»¥åœ¨è¿™é‡Œæ·»åŠ å…·ä½“çš„å‘Šè­¦é€»è¾‘
	// - å‘é€é‚®ä»¶é€šçŸ¥
	// - è°ƒç”¨å‘Šè­¦API
	// - å†™å…¥å‘Šè­¦æ—¥å¿—æ–‡ä»¶
	// - è§¦å‘ç›‘æ§ç³»ç»Ÿå‘Šè­¦
}

// GetStatus è·å–è°ƒåº¦å™¨çŠ¶æ€
func (is *IntegrityScheduler) GetStatus() map[string]interface{} {
	is.mutex.Lock()
	defer is.mutex.Unlock()

	return map[string]interface{}{
		"running":       is.running,
		"checkInterval": is.checkInterval.String(),
		"dependencies": map[string]bool{
			"tcpMonitor":         is.tcpMonitor != nil,
			"sessionManager":     is.sessionManager != nil,
			"deviceGroupManager": is.deviceGroupManager != nil,
		},
	}
}

// å…¨å±€è°ƒåº¦å™¨å®ä¾‹
var (
	globalIntegrityScheduler     *IntegrityScheduler
	globalIntegritySchedulerOnce sync.Once
)

// GetGlobalIntegrityScheduler è·å–å…¨å±€æ•°æ®å®Œæ•´æ€§æ£€æŸ¥è°ƒåº¦å™¨
func GetGlobalIntegrityScheduler() *IntegrityScheduler {
	globalIntegritySchedulerOnce.Do(func() {
		// é»˜è®¤æ¯30åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
		globalIntegrityScheduler = NewIntegrityScheduler(30 * time.Minute)
		logger.Info("IntegrityScheduler: å…¨å±€æ•°æ®å®Œæ•´æ€§æ£€æŸ¥è°ƒåº¦å™¨å·²åˆå§‹åŒ–")
	})
	return globalIntegrityScheduler
}
