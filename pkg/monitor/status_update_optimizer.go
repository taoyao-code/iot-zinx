// Package monitor è®¾å¤‡çŠ¶æ€æ›´æ–°ä¼˜åŒ–å™¨
// è§£å†³å¤šå±‚çº§é‡å¤è°ƒç”¨è®¾å¤‡çŠ¶æ€æ›´æ–°çš„æ€§èƒ½é—®é¢˜
package monitor

import (
	"fmt"
	"sync"
	"sync/atomic"
	"time"

	"github.com/bujia-iot/iot-zinx/internal/infrastructure/logger"
	"github.com/bujia-iot/iot-zinx/pkg/constants"
	"github.com/sirupsen/logrus"
)

// StatusUpdateOptimizer è®¾å¤‡çŠ¶æ€æ›´æ–°ä¼˜åŒ–å™¨
// é€šè¿‡å»é‡ã€å»¶è¿Ÿæ›´æ–°ã€æ‰¹é‡å¤„ç†ç­‰æ–¹å¼ä¼˜åŒ–è®¾å¤‡çŠ¶æ€æ›´æ–°æ€§èƒ½
type StatusUpdateOptimizer struct {
	// å»é‡æœºåˆ¶ï¼šé˜²æ­¢çŸ­æ—¶é—´å†…é‡å¤æ›´æ–°åŒä¸€è®¾å¤‡çŠ¶æ€
	lastUpdateStatus map[string]constants.DeviceStatus // deviceID -> æœ€åçŠ¶æ€
	lastUpdateTime   map[string]time.Time              // deviceID -> æœ€åæ›´æ–°æ—¶é—´
	mutex            sync.RWMutex

	// æ‰¹é‡æ›´æ–°æœºåˆ¶
	pendingUpdates   map[string]*StatusUpdate // deviceID -> å¾…æ›´æ–°çŠ¶æ€
	batchMutex       sync.Mutex
	batchUpdateTimer *time.Timer

	// é…ç½®å‚æ•°
	dedupInterval time.Duration                        // å»é‡é—´éš”ï¼Œé»˜è®¤1ç§’
	batchInterval time.Duration                        // æ‰¹é‡æ›´æ–°é—´éš”ï¼Œé»˜è®¤500æ¯«ç§’
	updateFunc    constants.UpdateDeviceStatusFuncType // å®é™…çŠ¶æ€æ›´æ–°å‡½æ•°

	// æ€§èƒ½ç»Ÿè®¡ï¼ˆä½¿ç”¨atomicç¡®ä¿å¹¶å‘å®‰å…¨ï¼‰
	stats *OptimizerStats

	// é…ç½®ç®¡ç†å™¨ï¼ˆå¯é€‰ï¼‰
	configManager *ConfigManager
}

// OptimizerStats ä¼˜åŒ–å™¨æ€§èƒ½ç»Ÿè®¡
type OptimizerStats struct {
	TotalRequests    int64     // æ€»è¯·æ±‚æ•°
	DeduplicatedReqs int64     // å»é‡çš„è¯·æ±‚æ•°
	ExecutedUpdates  int64     // å®é™…æ‰§è¡Œçš„æ›´æ–°æ•°
	BatchCount       int64     // æ‰¹é‡æ›´æ–°æ¬¡æ•°
	AvgBatchSize     int64     // å¹³å‡æ‰¹é‡å¤§å°
	StartTime        time.Time // ç»Ÿè®¡å¼€å§‹æ—¶é—´
}

// StatusUpdate çŠ¶æ€æ›´æ–°ä¿¡æ¯
type StatusUpdate struct {
	DeviceID  string
	Status    constants.DeviceStatus // ğŸ”§ çŠ¶æ€é‡æ„ï¼šä½¿ç”¨ç±»å‹å®‰å…¨çš„è®¾å¤‡çŠ¶æ€
	Timestamp time.Time
	Source    string // æ›´æ–°æ¥æºï¼šheartbeat, register, disconnectç­‰
}

// NewStatusUpdateOptimizer åˆ›å»ºçŠ¶æ€æ›´æ–°ä¼˜åŒ–å™¨
func NewStatusUpdateOptimizer(updateFunc constants.UpdateDeviceStatusFuncType) *StatusUpdateOptimizer {
	optimizer := &StatusUpdateOptimizer{
		lastUpdateStatus: make(map[string]constants.DeviceStatus),
		lastUpdateTime:   make(map[string]time.Time),
		pendingUpdates:   make(map[string]*StatusUpdate),
		dedupInterval:    1 * time.Second,        // 1ç§’å†…é‡å¤çŠ¶æ€æ›´æ–°ä¼šè¢«å»é‡
		batchInterval:    500 * time.Millisecond, // 500æ¯«ç§’æ‰¹é‡æ›´æ–°ä¸€æ¬¡
		updateFunc:       updateFunc,
		stats: &OptimizerStats{
			StartTime: time.Now(),
		},
	}

	logger.Info("è®¾å¤‡çŠ¶æ€æ›´æ–°ä¼˜åŒ–å™¨å·²åˆå§‹åŒ–")
	return optimizer
}

// UpdateDeviceStatus ä¼˜åŒ–çš„è®¾å¤‡çŠ¶æ€æ›´æ–°æ–¹æ³•
// source: æ›´æ–°æ¥æºï¼Œç”¨äºè°ƒè¯•å’Œç»Ÿè®¡ï¼ˆå¦‚ï¼šheartbeat, register, disconnectç­‰ï¼‰
func (o *StatusUpdateOptimizer) UpdateDeviceStatus(deviceID string, status constants.DeviceStatus, source string) {
	if deviceID == "" {
		return
	}

	// ç»Ÿè®¡æ€»è¯·æ±‚æ•°
	atomic.AddInt64(&o.stats.TotalRequests, 1)

	now := time.Now()

	// 1. æ£€æŸ¥æ˜¯å¦éœ€è¦å»é‡
	if o.shouldDeduplicate(deviceID, status, now) {
		atomic.AddInt64(&o.stats.DeduplicatedReqs, 1)
		logger.WithFields(logrus.Fields{
			"deviceID": deviceID,
			"status":   status,
			"source":   source,
		}).Debug("è®¾å¤‡çŠ¶æ€æ›´æ–°è¢«å»é‡ä¼˜åŒ–")
		return
	}

	// 2. æ·»åŠ åˆ°æ‰¹é‡æ›´æ–°é˜Ÿåˆ—
	o.addToPendingUpdates(deviceID, status, source, now)

	// 3. å¯åŠ¨æˆ–é‡ç½®æ‰¹é‡æ›´æ–°å®šæ—¶å™¨
	o.scheduleTheBatchUpdate()
}

// shouldDeduplicate æ£€æŸ¥æ˜¯å¦åº”è¯¥å»é‡
func (o *StatusUpdateOptimizer) shouldDeduplicate(deviceID string, status constants.DeviceStatus, now time.Time) bool {
	o.mutex.RLock()
	defer o.mutex.RUnlock()

	lastStatus, hasLastStatus := o.lastUpdateStatus[deviceID]
	lastTime, hasLastTime := o.lastUpdateTime[deviceID]

	// å¦‚æœçŠ¶æ€ç›¸åŒä¸”åœ¨å»é‡é—´éš”å†…ï¼Œåˆ™å»é‡
	if hasLastStatus && hasLastTime {
		if lastStatus == status && now.Sub(lastTime) < o.dedupInterval {
			return true
		}
	}

	return false
}

// addToPendingUpdates æ·»åŠ åˆ°å¾…æ›´æ–°é˜Ÿåˆ—
func (o *StatusUpdateOptimizer) addToPendingUpdates(deviceID string, status constants.DeviceStatus, source string, timestamp time.Time) {
	o.batchMutex.Lock()
	defer o.batchMutex.Unlock()

	// æ›´æ–°å¾…å¤„ç†åˆ—è¡¨ï¼ˆå¦‚æœè®¾å¤‡å·²å­˜åœ¨ï¼Œè¦†ç›–ä¸ºæœ€æ–°çŠ¶æ€ï¼‰
	o.pendingUpdates[deviceID] = &StatusUpdate{
		DeviceID:  deviceID,
		Status:    status,
		Timestamp: timestamp,
		Source:    source,
	}
}

// scheduleTheBatchUpdate è°ƒåº¦æ‰¹é‡æ›´æ–°
func (o *StatusUpdateOptimizer) scheduleTheBatchUpdate() {
	o.batchMutex.Lock()
	defer o.batchMutex.Unlock()

	// å¦‚æœå®šæ—¶å™¨å·²å­˜åœ¨ï¼Œé‡ç½®å®ƒ
	if o.batchUpdateTimer != nil {
		o.batchUpdateTimer.Stop()
	}

	// åˆ›å»ºæ–°çš„å®šæ—¶å™¨
	o.batchUpdateTimer = time.AfterFunc(o.batchInterval, func() {
		o.executeBatchUpdate()
	})
}

// executeBatchUpdate æ‰§è¡Œæ‰¹é‡æ›´æ–°
func (o *StatusUpdateOptimizer) executeBatchUpdate() {
	o.batchMutex.Lock()
	updatesCopy := make(map[string]*StatusUpdate)
	for k, v := range o.pendingUpdates {
		updatesCopy[k] = v
	}
	o.pendingUpdates = make(map[string]*StatusUpdate) // æ¸…ç©ºå¾…æ›´æ–°åˆ—è¡¨
	o.batchMutex.Unlock()

	if len(updatesCopy) == 0 {
		return
	}

	batchSize := int64(len(updatesCopy))

	// æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
	atomic.AddInt64(&o.stats.BatchCount, 1)
	atomic.AddInt64(&o.stats.ExecutedUpdates, batchSize)

	// è®¡ç®—å¹³å‡æ‰¹é‡å¤§å°
	totalBatches := atomic.LoadInt64(&o.stats.BatchCount)
	totalExecuted := atomic.LoadInt64(&o.stats.ExecutedUpdates)
	if totalBatches > 0 {
		atomic.StoreInt64(&o.stats.AvgBatchSize, totalExecuted/totalBatches)
	}

	// æŒ‰è®¾å¤‡ç»„åˆ†ç»„æ›´æ–°ï¼Œä¼˜åŒ–ç›¸å…³è®¾å¤‡çš„æ›´æ–°
	deviceGroups := make(map[string][]*StatusUpdate) // ICCID -> æ›´æ–°åˆ—è¡¨
	noGroupDevices := make([]*StatusUpdate, 0)       // æ— ç»„è®¾å¤‡çš„æ›´æ–°åˆ—è¡¨

	// åˆ†ç»„è®¾å¤‡
	sessionManager := GetSessionManager()
	for _, update := range updatesCopy {
		if session, exists := sessionManager.GetSession(update.DeviceID); exists && session.ICCID != "" {
			// æœ‰ICCIDçš„è®¾å¤‡ï¼ŒæŒ‰ICCIDåˆ†ç»„
			iccid := session.ICCID
			if _, ok := deviceGroups[iccid]; !ok {
				deviceGroups[iccid] = make([]*StatusUpdate, 0)
			}
			deviceGroups[iccid] = append(deviceGroups[iccid], update)
		} else {
			// æ— ICCIDçš„è®¾å¤‡ï¼Œæ”¾å…¥æ— ç»„åˆ—è¡¨
			noGroupDevices = append(noGroupDevices, update)
		}
	}

	// æ‰§è¡Œæ‰¹é‡æ›´æ–°
	sources := make(map[string]int) // ç»Ÿè®¡æ›´æ–°æ¥æº

	// 1. å…ˆå¤„ç†æœ‰ç»„è®¾å¤‡
	for iccid, updates := range deviceGroups {
		// è®°å½•ç»„çŠ¶æ€å˜åŒ–
		var activeCount int
		var offlineCount int

		// æŒ‰è®¾å¤‡ç»„å¤„ç†
		for _, update := range updates {
			// æ›´æ–°å»é‡è®°å½•
			o.mutex.Lock()
			o.lastUpdateStatus[update.DeviceID] = update.Status
			o.lastUpdateTime[update.DeviceID] = update.Timestamp
			o.mutex.Unlock()

			// æ‰§è¡Œå®é™…æ›´æ–°
			if o.updateFunc != nil {
				o.updateFunc(update.DeviceID, update.Status)
			}

			// ç»Ÿè®¡çŠ¶æ€
			if update.Status == constants.DeviceStatusOnline {
				activeCount++
			} else if update.Status == constants.DeviceStatusOffline {
				offlineCount++
			}

			// ç»Ÿè®¡æ›´æ–°æ¥æº
			sources[update.Source]++
		}

		// è®°å½•ç»„çŠ¶æ€æ—¥å¿—
		if len(updates) > 1 {
			logger.WithFields(logrus.Fields{
				"iccid":        iccid,
				"totalDevices": len(updates),
				"active":       activeCount,
				"offline":      offlineCount,
			}).Debug("è®¾å¤‡ç»„çŠ¶æ€æ‰¹é‡æ›´æ–°")
		}
	}

	// 2. å¤„ç†æ— ç»„è®¾å¤‡
	for _, update := range noGroupDevices {
		// æ›´æ–°å»é‡è®°å½•
		o.mutex.Lock()
		o.lastUpdateStatus[update.DeviceID] = update.Status
		o.lastUpdateTime[update.DeviceID] = update.Timestamp
		o.mutex.Unlock()

		// æ‰§è¡Œå®é™…æ›´æ–°
		if o.updateFunc != nil {
			o.updateFunc(update.DeviceID, update.Status)
		}

		// ç»Ÿè®¡æ›´æ–°æ¥æº
		sources[update.Source]++
	}

	// è®°å½•æ‰¹é‡æ›´æ–°æ—¥å¿—
	logger.WithFields(logrus.Fields{
		"batchSize":    batchSize,
		"sources":      sources,
		"avgBatchSize": atomic.LoadInt64(&o.stats.AvgBatchSize),
	}).Debug("æ‰§è¡Œæ‰¹é‡è®¾å¤‡çŠ¶æ€æ›´æ–°")
}

// GetStats è·å–ä¼˜åŒ–å™¨ç»Ÿè®¡ä¿¡æ¯
func (o *StatusUpdateOptimizer) GetStats() map[string]interface{} {
	o.mutex.RLock()
	o.batchMutex.Lock()

	totalReqs := atomic.LoadInt64(&o.stats.TotalRequests)
	dedupReqs := atomic.LoadInt64(&o.stats.DeduplicatedReqs)
	executedUpdates := atomic.LoadInt64(&o.stats.ExecutedUpdates)
	batchCount := atomic.LoadInt64(&o.stats.BatchCount)
	avgBatchSize := atomic.LoadInt64(&o.stats.AvgBatchSize)

	// è®¡ç®—å»é‡æ•ˆç‡
	dedupRatio := float64(0)
	if totalReqs > 0 {
		dedupRatio = float64(dedupReqs) / float64(totalReqs) * 100
	}

	// è®¡ç®—è¿è¡Œæ—¶é—´
	uptime := time.Since(o.stats.StartTime)

	stats := map[string]interface{}{
		"trackedDevices":   len(o.lastUpdateStatus),
		"pendingUpdates":   len(o.pendingUpdates),
		"totalRequests":    totalReqs,
		"deduplicatedReqs": dedupReqs,
		"executedUpdates":  executedUpdates,
		"batchCount":       batchCount,
		"avgBatchSize":     avgBatchSize,
		"dedupRatio":       fmt.Sprintf("%.2f%%", dedupRatio),
		"dedupInterval":    o.dedupInterval.String(),
		"batchInterval":    o.batchInterval.String(),
		"uptime":           uptime.String(),
	}

	o.batchMutex.Unlock()
	o.mutex.RUnlock()

	return stats
}

// FlushPendingUpdates ç«‹å³åˆ·æ–°æ‰€æœ‰å¾…æ›´æ–°çŠ¶æ€ï¼ˆç”¨äºç³»ç»Ÿå…³é—­æ—¶ï¼‰
func (o *StatusUpdateOptimizer) FlushPendingUpdates() {
	if o.batchUpdateTimer != nil {
		o.batchUpdateTimer.Stop()
	}
	o.executeBatchUpdate()
}

// SetDedupInterval è®¾ç½®å»é‡é—´éš”
func (o *StatusUpdateOptimizer) SetDedupInterval(interval time.Duration) {
	o.mutex.Lock()
	defer o.mutex.Unlock()
	o.dedupInterval = interval
}

// SetBatchInterval è®¾ç½®æ‰¹é‡æ›´æ–°é—´éš”
func (o *StatusUpdateOptimizer) SetBatchInterval(interval time.Duration) {
	o.mutex.Lock()
	defer o.mutex.Unlock()
	o.batchInterval = interval
}

// Stop åœæ­¢ä¼˜åŒ–å™¨å¹¶åˆ·æ–°å¾…å¤„ç†çš„æ›´æ–°
func (o *StatusUpdateOptimizer) Stop() {
	logger.Info("æ­£åœ¨åœæ­¢è®¾å¤‡çŠ¶æ€æ›´æ–°ä¼˜åŒ–å™¨...")

	// åœæ­¢å®šæ—¶å™¨
	if o.batchUpdateTimer != nil {
		o.batchUpdateTimer.Stop()
	}

	// åˆ·æ–°æ‰€æœ‰å¾…å¤„ç†çš„æ›´æ–°
	o.FlushPendingUpdates()

	// æ‰“å°æœ€ç»ˆç»Ÿè®¡ä¿¡æ¯
	stats := o.GetStats()
	logger.WithFields(logrus.Fields{
		"stats": stats,
	}).Info("è®¾å¤‡çŠ¶æ€æ›´æ–°ä¼˜åŒ–å™¨å·²åœæ­¢")
}
