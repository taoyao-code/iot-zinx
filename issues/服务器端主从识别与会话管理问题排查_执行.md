**背景**:
在主从设备架构中，持有 SIM 卡的设备（我们称之为主设备或 SIM 卡设备）建立网络连接并提供 ICCID。该设备本身，以及通过线缆串联的其他分机设备，都通过 DNY 协议的 0x20 命令（设备注册）在该 ICCID 和共享的 TCP 连接上进行各自的注册，并上报其唯一的 PhysicalID。当前遇到的问题是，应用层可能无法有效地通过主设备的 ICCID 及分机的唯一标识查找到这些分机的会话。

**核心问题点**:

1.  **`DeviceRegisterHandler` (处理 0x20 命令)**：需要确保为每个通过 0x20 注册的设备（基于其`PhysicalID`）生成一个在服务器端唯一的`DeviceID`，并用此`DeviceID`创建独立的`monitor.DeviceSession`。
2.  **会话管理与分组 (`SessionManager`, `DeviceGroupManager`)**：
    - 必须确保每个通过 0x20 注册的设备（即使共享 ICCID 和 TCP 连接）都在`SessionManager`中拥有自己独立的`DeviceSession`，以其服务器端生成的唯一`DeviceID`为键。
    - 这些独立的`DeviceSession`需要被正确地组织在以`ICCID`标识的设备组中，由`DeviceGroupManager`管理。
3.  **连接与设备映射 (`TCPMonitor`)**：
    - `TCPMonitor`需要准确地维护共享物理连接（`ConnID`）与该连接上所有逻辑设备（`DeviceID`s）之间的双向映射关系。
    - 当共享连接断开时，必须正确清理所有关联的逻辑设备映射，并通知`SessionManager`处理这些设备的会话状态。
4.  **应用层查找逻辑**：应用层应能通过`SessionManager.GetAllSessionsByICCID(iccid)`获取指定 ICCID 下的所有设备会话，然后根据每个会话的`DeviceID`字段来筛选和定位到特定的分机设备。

**计划步骤**:

> 注意：此计划基于当前理解，即主要问题在于处理通过 0x20 命令注册的、共享 ICCID 和连接的多个设备。

1.  **审阅并增强 `internal/infrastructure/zinx_server/handlers/device_register_handler.go` (`DeviceRegisterHandler.Handle`)**:

    - **动作**: 当接收到 0x20 设备注册命令时：
      - 从 0x20 负载中提取`PhysicalID`。
      - 基于`PhysicalID`生成一个服务器端唯一的`deviceID`字符串（例如，使用`physical_id_helper.Uint32ToDeviceIDString(physicalID)`或类似机制，确保格式统一，如`04A228CD`）。此`deviceID`将作为`SessionManager`中会话的主键。
      - 从连接属性中获取与此连接关联的`ICCID` (例如 `conn.GetProperty(constants.PropKeyICCID)`).
      - 调用 `SessionManager.CreateSession(deviceID, conn)` 来创建或更新设备会话。确保传入的`conn`对象包含了必要的`ICCID`属性。
    - **验证**: 确认此处理器为每个不同的`PhysicalID`（即使在同一连接和 ICCID 下）都创建了具有唯一`deviceID`的会话。

2.  **审阅并确认 `pkg/monitor/session_manager.go` (`SessionManager`) 的行为**:

    - **`CreateSession(deviceID string, conn ziface.IConnection)` 方法**:
      - 确认它能从`conn`参数中正确提取`ICCID`。
      - 确认它使用传入的`deviceID`作为键，在内部的会话存储（如`m.sessions`）中存储`DeviceSession`对象。
      - 确认它调用`m.deviceGroupManager.AddDeviceToGroup(iccid, deviceID, sessionData)`，将当前`deviceID`及其会话数据正确关联到对应的`ICCID`设备组中。
    - **`GetAllSessionsByICCID(iccid string) map[string]*DeviceSession` 方法**:
      - 确认此方法能从`deviceGroupManager`中可靠地获取并返回与指定`ICCID`关联的所有`DeviceSession`对象。这将是应用层查找同一 SIM 卡下所有设备的主要入口。
    - **`GetSession(deviceID string) (*DeviceSession, bool)` 方法**:
      - 确认此方法能根据唯一的`deviceID`直接查找到对应的会话。

3.  **审阅并确认 `pkg/monitor/device_group.go` (`DeviceGroupManager`) 的行为**:

    - **`AddDeviceToGroup(iccid, deviceID string, sessionData *DeviceSession)`**:
      - 确认它能在一个`ICCID`组内正确添加或更新由`deviceID`标识的设备及其会话数据。
    - **`GetAllDevicesInGroup(iccid string) map[string]*DeviceSession`**:
      - 确认它能返回指定`ICCID`组内的所有设备及其会话。
    - **`RemoveDeviceFromGroup(iccid, deviceID string)`**:
      - 确认当设备会话被移除时，此方法能正确地将其从对应的`ICCID`组中移除。

4.  **审阅并增强 `pkg/monitor/tcp_monitor.go` (`TCPMonitor`) 的映射逻辑 (最终实现)**:

    - **核心映射结构**:
      - `deviceIdToConnMap: map[string]uint64`: 存储 `DeviceID` 到其物理连接 `ConnID` 的映射。
      - `connIdToDeviceIdsMap: map[uint64]map[string]struct{}`: 存储 `ConnID` 到该连接上所有 `DeviceID` 集合的映射 (使用 `struct{}` 作为 set 的值以节省空间)。
      - (可能还存在旧的 `connIdToDeviceIdMap: map[uint64]string` 用于特定单设备场景或兼容性，但主要逻辑围绕新的映射展开)。
    - **`BindDeviceIdToConnection(deviceID string, conn ziface.IConnection, isMasterDevice bool)` 方法 (统一逻辑)**:
      - 获取 `connID`。
      - **加锁**: 使用互斥锁 (如 `tm.bindMutex` 或 `tm.mu`) 保护后续的映射操作。
      - **处理设备切换**: 如果此 `deviceID` 之前已绑定到其他连接，则从旧连接的 `connIdToDeviceIdsMap` 集合中移除该 `deviceID`。
      - 更新 `deviceIdToConnMap`: 将 `deviceID` 映射到新的 `connID`。
      - 更新 `connIdToDeviceIdsMap`:
        - 获取或创建 `connID` 对应的 `DeviceID` 集合。
        - 将当前 `deviceID` 添加到此集合中。
      - (可选，根据具体实现) 如果是主设备，可能还会更新特定的主设备映射 (如 `masterConnectionMap`)。
      - 设置连接属性，例如 `conn.SetProperty(constants.PropKeyDeviceId, deviceID)`。
    - **`OnConnectionClosed(conn ziface.IConnection)` 方法 (统一逻辑)**:
      - 获取关闭连接的 `connID` 和 `remoteAddr`。
      - **加锁**: 使用互斥锁保护后续的映射操作。
      - **收集待处理的 DeviceID**:
        - 从 `connIdToDeviceIdsMap` 中获取与此 `connID` 关联的所有 `DeviceID`。
        - (可选，为兼容旧逻辑) 如果上述集合为空，并且存在旧的 `connIdToDeviceIdMap`，则从中获取 `DeviceID`。
      - **处理每个受影响的 DeviceID**:
        - 遍历收集到的 `DeviceID` 列表。
        - 从 `deviceIdToConnMap` 中删除该 `DeviceID` 的条目 (如果它确实映射到当前关闭的 `connID`)。
        - 通知 `SessionManager.SuspendSession(deviceID)`。
        - (可选) 调用设备监控相关的回调，如 `deviceMonitor.OnDeviceDisconnect(deviceID)`。
        - (可选) 调用设备状态更新函数。
      - **清理连接映射**:
        - 从 `connIdToDeviceIdsMap` 中删除该 `connID` 的条目。
        - (可选) 从旧的 `connIdToDeviceIdMap` 中删除该 `connID` 的条目。
        - (可选) 如果是主设备连接，清理主设备相关映射。
    - **`GetConnectionByDeviceID(deviceID string) (ziface.IConnection, bool)` 方法**:
      - 通过 `deviceIdToConnMap` 查找 `ConnID`，然后通过 `Zinx` 的连接管理器获取 `IConnection` 对象。
    - **验证**: 确保 `TCPMonitor` 能够正确处理单个连接上的多个设备注册，并在连接建立和关闭时准确更新所有相关映射。确保线程安全。

5.  **审阅 `pkg/protocol/sender.go` (或其他负责命令发送的模块，如 `getActualConnectionForDevice`)**:
    - **动作**: 当需要向一个特定的`deviceID`（分机）发送命令时：
      - 应首先调用`TCPMonitor.GetConnectionByDeviceID(deviceID)`来获取该设备实际所在的物理连接。
      - 然后通过获取到的连接对象发送命令。
    - **验证**: 确保命令能够基于目标设备的`deviceID`正确路由到共享的物理连接。

## 对话摘要 (截至 2025 年 6 月 12 日)

**任务描述:**
调查服务器无法通过 ICCID 找到设备会话的错误，特别是针对“从属”或“扩展”设备。这些设备与一个“主”（持有 SIM 卡）设备共享单个 TCP 连接和 ICCID。所有设备都使用 DNY 0x20 命令单独注册，每个设备报告其唯一的 `PhysicalID`。目标是确保服务器为共享 ICCID 和连接下的每个 `PhysicalID`（派生为唯一的 `DeviceID` 字符串）创建不同的 `monitor.DeviceSession` 实例。这应该允许找到这些单独的设备会话并与之交互。重点是正确处理共享连接上的多个 0x20 注册，并确保 `TCPMonitor` 正确管理这些共享连接资源。

**已完成:**

1.  **初步问题澄清和日志审查:** 确认在相同的 `ConnID` 和 `ICCID` 下存在多个 0x20 注册（不同的 `PhysicalID`）。
2.  **计划制定和更新:**
    - 采纳了一个专注于增强 `DeviceGroupManager`、`SessionManager` 和 `TCPMonitor` 以处理一个 `ICCID` 下多个 `DeviceID` 的共享连接的计划。
    - 多次更新了执行计划文档 (`issues/服务器端主从识别与会话管理问题排查_执行.md`)。
3.  **`DeviceRegisterHandler` 修改和审查 (`internal/infrastructure/zinx_server/handlers/device_register_handler.go`):**
    - 修复了 `handleDeviceRegister` 中的 ICCID 检索。确保使用 `DeviceID`（源自 `PhysicalID`）进行会话创建和 `TCPMonitor.BindDeviceIdToConnection`。
4.  **`SessionManager` 和 `DeviceGroupManager` 审查:**
    - 验证了它们现有的按 `DeviceID` 创建会话和按 `ICCID` 分组的逻辑是兼容的。
5.  **`TCPMonitor` 审查和广泛修改 (`pkg/monitor/tcp_monitor.go`):**
    - 应用了重大修改来管理共享连接。
    - 关键变更（基于 `tcp_monitor.go` 的最新代码）:
      - `deviceIdToConnMap` 使用 `map[string]uint64` (DeviceID 到 ConnID)。
      - `connIdToDeviceIdsMap` 使用 `map[uint64]map[string]struct{}` (ConnID 到一组 DeviceID)。
      - 旧的 `connIdToDeviceIdMap` (`map[uint64]string`) 仍用于特定的单设备每连接场景或回退。
      - **`BindDeviceIdToConnection` (统一逻辑)**:
        - 更新 `deviceIdToConnMap` (DeviceID -> ConnID)。
        - 更新 `connIdToDeviceIdsMap` (ConnID -> DeviceID 集合)。
        - 处理设备切换：从 `connIdToDeviceIdsMap` 中的旧连接集合中移除设备。
      - **`OnConnectionClosed` (统一逻辑)**:
        - 使用 `connIdToDeviceIdsMap` 和 `connIdToDeviceIdMap` 识别关闭连接上的所有 `DeviceID`。
        - 如果这些 `DeviceID` 映射到 `closedConnID`，则从 `deviceIdToConnMap` 中清除它们。
        - 为每个受影响的 `DeviceID` 通知 `SessionManager`。
        - 清理 `closedConnID` 的 `connIdToDeviceIdsMap` 和 `connIdToDeviceIdMap` 条目。

[模式：执行]

好的，我们来进一步排查潜在问题，并对方案进行完善和统一，然后将这些更新写入到您的执行文档 `服务器端主从识别与会话管理问题排查_执行.md` 中。

**核心原则：**

- **高内聚、低耦合**：模块功能应集中，模块间依赖应最小。
- **单一职责原则 (SRP)**：一个类或模块应该有且只有一个引起它变化的原因。
- **并发安全**：所有共享资源（尤其是 map）的访问必须是线程安全的。
- **代码清晰与可维护性**：逻辑清晰，易于理解和修改。

**针对 tcp_monitor.go 的进一步潜在问题排查与修复方案：**

1.  **`sync.Map` 的错误使用**:

    - **问题**: 在您提供的 `tcp_monitor.go` 的 `OnConnectionClosed` 函数（以及可能的其他地方）中，对 `sync.Map` 类型的字段（如 `deviceIdToConnMap`, `connIdToDeviceIdMap`, `connIdToDeviceIdsMap`）进行了直接的 map 索引访问（例如 `m.connIdToDeviceIdsMap[closedConnID]`）和 `delete()` 操作。这是错误的，`sync.Map` 必须使用其特有的方法如 `Load()`, `Store()`, `Delete()`, `Range()`。
    - **修复**: 所有对 `sync.Map` 实例的访问都必须修改为使用其标准方法。同时，从 `Load()` 获取的值需要进行类型断言。
    - **示例 (错误)**: `if val, exists := m.mySyncMap[key]; exists {...}`
    - **示例 (正确)**:
      ```go
      if valActual, loaded := m.mySyncMap.Load(key); loaded {
          if val, ok := valActual.(ExpectedType); ok {
              // ... use val ...
          } else {
              // ... handle type assertion error ...
          }
      }
      ```

2.  **`connIdToDeviceIdsMap` 中集合的类型**:

    - **问题**: `connIdToDeviceIdsMap` 定义为 `sync.Map // map[uint64]map[string]bool`。使用 `map[string]struct{}` 作为集合类型更为惯用且内存效率略高。
    - **修复**: 建议将 `connIdToDeviceIdsMap` 内部存储的 set 类型从 `map[string]bool` 改为 `map[string]struct{}`。

3.  **`BindDeviceIdToConnection` 函数未实现**:

    - **问题**: 此核心函数的逻辑在当前代码中是空的（只有注释）。
    - **修复**: 完整实现此函数，确保其原子性、正确处理设备切换连接（从旧连接的设备集合中移除，添加到新连接的设备集合），并更新所有相关的映射 (`deviceIdToConnMap`, `connIdToDeviceIdsMap`)。

4.  **冗余的连接关闭处理器**:

    - **问题**: 代码中可能存在 `handleMasterConnectionClosed` 和 `handleSingleDeviceConnectionClosed` 等函数，而新的 `OnConnectionClosed` 应该是统一的处理器。
    - **修复**: 移除旧的、冗余的连接关闭处理函数，确保 `OnConnectionClosed` 是处理连接关闭事件的唯一入口点 (Single Responsibility Principle)。

5.  **`masterConnectionMap` 和 `connIdToDeviceIdMap` 的角色与生命周期**:

    - **问题**: 这两个映射用于追踪“主设备”或“首要设备”。它们在 `BindDeviceIdToConnection` 中的更新逻辑以及在 `OnConnectionClosed` 中的清理逻辑需要明确和完整。当前 `OnConnectionClosed` 中 `masterConnectionMap` 的清理是被注释掉的。
    - **修复**:
      - 如果这两个映射是必需的，确保在 `BindDeviceIdToConnection` 中有明确的逻辑来决定何时以及如何更新它们。
      - 在 `OnConnectionClosed` 中，必须添加对 `masterConnectionMap` 的清理逻辑（如果保留该 map）。
      - **考量**: 重新评估这两个映射的必要性。如果“主设备”信息可以从设备自身的属性或 `DeviceRegisterHandler` 设置在连接上的特定属性 (`conn.GetProperty`) 中获取，可以考虑是否简化 `TCPMonitor` 的状态。但若 `TCPMonitor` 内部逻辑频繁依赖此信息，则保留映射是合理的。当前方案倾向于保留并正确管理它们。

6.  **`SessionManager` 的注入/初始化**:

    - **问题**: `TCPMonitor` 在 `OnConnectionClosed` 中调用 `m.sessionManager.HandleDeviceDisconnect(deviceID)`，但 `sessionManager` 成员的初始化在 `GetGlobalMonitor` 中并未体现。
    - **修复**: 为 `TCPMonitor` 提供一个设置 `SessionManager` 的方法（例如 `SetSessionManager(sm *SessionManager)`），并在系统初始化时调用它。

7.  **并发锁的范围和粒度**:
    - **问题**: `bindMutex` (RWMutex) 用于保护对多个映射的复合操作。需要确保所有涉及共享状态修改的路径都被正确地加锁保护。
    - **修复**: 仔细审查 `BindDeviceIdToConnection` 和 `OnConnectionClosed` 的实现，确保锁在必要时获取，并在操作完成后释放。`sync.Map` 本身处理单个操作的并发，但跨多个 `sync.Map` 或依赖于多个 `sync.Map` 状态的逻辑序列仍需外部锁。

现在，我将把这些分析和修复方案整合到您的执行文档中。

[模式：执行]
好的，我将根据您的要求，结合之前的分析和计划，对 tcp_monitor.go 文件进行修复和统一化处理，并尽可能遵循高内聚、低耦合、单一职责等设计原则。

**核心思路：**

1.  **统一映射类型**：将 `sync.Map` 替换为带互斥锁 (`sync.RWMutex`) 的普通 `map`，以便进行更细致的并发控制和类型安全。`sync.Map` 适用于读多写少的场景，但在我们的场景中，写操作（绑定、解绑）也比较关键，且类型断言繁琐。
2.  **职责明确**：
    - `TCPMonitor` 主要负责 `DeviceID` 与 `ConnID` 之间的映射关系管理。
    - 会话状态的更新（如 `Online`, `Offline`）应主要由 `SessionManager` 负责，`TCPMonitor` 在连接事件发生时通知 `SessionManager`。
3.  **简化函数**：移除冗余或逻辑重叠的函数，如 `setDeviceProperties`。合并 `handleMasterDeviceBinding` 和 `handleSlaveDeviceBinding` 的相似逻辑。
4.  **连接关闭处理统一化**：`OnConnectionClosed` 应成为处理连接关闭的唯一入口，内部逻辑清晰地处理所有关联设备的清理。移除 `handleMasterConnectionClosed` 和 `handleSingleDeviceConnectionClosed`，将它们的逻辑整合进 `OnConnectionClosed` 的调用者（如果适用）。从您最新的代码看，`OnConnectionClosed` 已经变得相当全面。
5.  **`sync.Map` 与 `map[uint64]map[string]struct{}` 的选择**：
    - `connIdToDeviceIdsMap` 当前声明为 `sync.Map`，但其值类型是 `map[string]bool` (或 `map[string]struct{}`)。当使用 `sync.Map` 时，其存储的值应该是线程安全的，或者在访问和修改这些值时需要额外的同步机制。
    - 如果 `map[string]struct{}` 的并发访问和修改不频繁，或者可以被外层锁（如 `bindMutex`）覆盖，则当前方式可行。
    - 为了更清晰和统一，我会将 `deviceIdToConnMap` 和 `connIdToDeviceIdsMap` 的底层实现都改为受 `bindMutex` 保护的普通 `map`。这简化了并发模型，所有对这些核心映射的访问都由一个统一的读写锁控制。
      - `deviceIdToConnMap: map[string]uint64` (存储 ConnID，而不是整个 IConnection 对象，以避免循环引用和简化序列化/调试)
      - `connIdToDeviceIdsMap: map[uint64]map[string]struct{}`
    - `connIdToDeviceIdMap` 和 `masterConnectionMap` 也可以类似处理。
6.  **移除 `setDeviceSpecificConnectionProperties`**: 此函数的唯一作用是 `conn.SetProperty(constants.PropKeyDeviceId, deviceId)`。这个操作更适合在 `DeviceRegisterHandler` 中根据业务逻辑（例如，是否是主设备首次注册）来决定是否设置以及设置什么。`TCPMonitor` 只负责映射，而不是直接修改连接的业务属性。如果确实需要在 `TCPMonitor` 中设置，也应在 `BindDeviceIdToConnection` 内部按需进行。为了减少耦合，我会倾向于将其移出 `TCPMonitor`。但根据现有计划，我们先移除其错误检查。
7.  **移除 `setDeviceProperties`**: 按计划移除。
8.  **`SessionManager` 的注入**: `TCPMonitor` 依赖 `SessionManager`。确保 `SessionManager` 实例被正确设置给 `globalMonitor.sessionManager`。

下面是具体的代码修改。

```go
package monitor

import (
    "sync"

    "github.com/yourorg/yourproject/ziface"
)

// TCPMonitor 负责管理设备与连接之间的映射关系。
type TCPMonitor struct {
    deviceIdToConnMap  map[string]uint64              // DeviceID 到 ConnID 的映射
    connIdToDeviceIdsMap map[uint64]map[string]struct{} // ConnID 到 DeviceID 集合的映射
    bindMutex           sync.RWMutex                   // 读写锁，用于保护映射的并发访问
    sessionManager      *SessionManager                // 会话管理器
}

// NewTCPMonitor 创建一个新的 TCPMonitor 实例。
func NewTCPMonitor(sm *SessionManager) *TCPMonitor {
    return &TCPMonitor{
        deviceIdToConnMap:  make(map[string]uint64),
        connIdToDeviceIdsMap: make(map[uint64]map[string]struct{}),
        sessionManager:      sm,
    }
}

// BindDeviceIdToConnection 绑定设备 ID 到物理连接，并处理设备切换逻辑。
func (tm *TCPMonitor) BindDeviceIdToConnection(deviceID string, conn ziface.IConnection, isMasterDevice bool) {
    newConnID := conn.GetConnID()

    tm.bindMutex.Lock()
    defer tm.bindMutex.Unlock()

    // 处理设备切换连接的逻辑
    if oldConnID, exists := tm.deviceIdToConnMap[deviceID]; exists && oldConnID != newConnID {
        // 从旧连接的设备集合中移除
        if deviceSet, ok := tm.connIdToDeviceIdsMap[oldConnID]; ok {
            delete(deviceSet, deviceID)
            // 如果旧连接的设备集合为空，则删除该集合
            if len(deviceSet) == 0 {
                delete(tm.connIdToDeviceIdsMap, oldConnID)
            }
        }
    }

    // 更新映射
    tm.deviceIdToConnMap[deviceID] = newConnID
    if _, ok := tm.connIdToDeviceIdsMap[newConnID]; !ok {
        tm.connIdToDeviceIdsMap[newConnID] = make(map[string]struct{})
    }
    tm.connIdToDeviceIdsMap[newConnID][deviceID] = struct{}{}
}

// OnConnectionClosed 处理连接关闭事件，清理所有关联的设备映射。
func (tm *TCPMonitor) OnConnectionClosed(conn ziface.IConnection) {
    closedConnID := conn.GetConnID()

    tm.bindMutex.Lock()
    defer tm.bindMutex.Unlock()

    // 获取与关闭的 ConnID 关联的所有 DeviceID
    if deviceSet, ok := tm.connIdToDeviceIdsMap[closedConnID]; ok {
        for deviceID := range deviceSet {
            // 从 deviceIdToConnMap 中删除该 DeviceID
            if tm.deviceIdToConnMap[deviceID] == closedConnID {
                delete(tm.deviceIdToConnMap, deviceID)
            }
            // 通知 SessionManager 处理设备断开
            tm.sessionManager.HandleDeviceDisconnect(deviceID)
        }
        // 清理 connIdToDeviceIdsMap 中的条目
        delete(tm.connIdToDeviceIdsMap, closedConnID)
    }
}

// GetConnectionByDeviceID 根据 DeviceID 获取实际的物理连接。
func (tm *TCPMonitor) GetConnectionByDeviceID(deviceID string) ziface.IConnection {
    tm.bindMutex.RLock()
    defer tm.bindMutex.RUnlock()

    if connID, ok := tm.deviceIdToConnMap[deviceID]; ok {
        // 从 Zinx 的全局连接管理器获取连接对象
        if conn := zinx.GlobalServer.GetConnMgr().Get(connID); conn != nil {
            return conn
        }
    }
    return nil
}
```

## 统一修改计划 (2025 年 6 月 12 日)

基于之前的讨论和目前 `pkg/monitor/tcp_monitor.go` 的重构进展，现制定统一修改计划如下：

**第一阶段：稳定 `pkg/monitor/tcp_monitor.go`**

1.  **代码审查与精炼 `pkg/monitor/tcp_monitor.go`：**
    - **`GetDeviceIdByConnId` 方法审查**：
      - **问题**：当前此方法返回单个 `DeviceID`，对于一个连接上存在多个设备的情况，其行为（返回第一个）可能不符合预期或不够明确。
      - **操作**：评估此方法在多设备场景下的确切需求。如果业务逻辑总是需要特定设备（例如，基于某种优先级的“主”设备概念，即使我们移除了旧的主从逻辑）或者需要所有设备，则调整此方法或建议调用方使用 `GetDeviceIdsByConnId` (plural)。明确此方法的协定。
      - **预期**：方法行为清晰，文档明确。
    - **`UpdateLastHeartbeatTime` 和 `UpdateDeviceStatus` 方法实现**：
      - **问题**：当前是占位符实现。
      - **操作**：根据单一职责原则，这些方法的核心逻辑（更新会话心跳、更新设备状态）应委托给 `SessionManager`。`TCPMonitor` 中仅保留必要的日志记录和对 `SessionManager` 的调用。
      - **预期**：`TCPMonitor` 职责清晰，状态更新逻辑由 `SessionManager` 处理。
    - **并发控制审查**：
      - **操作**：仔细检查所有方法，特别是涉及 `mapMutex` 的地方，确保锁的粒度、获取和释放时机正确，避免死锁和竞态条件。特别关注 `OnConnectionClosed` 和 `BindDeviceIdToConnection` 中的复杂逻辑。
      - **预期**：并发安全。
    - **`ForEachConnection` 方法优化**：
      - **问题**：当前实现中，在复制映射后过早释放了读锁，但在循环内部访问 `m.connManager.Get(connID)` 时，如果 `connManager` 本身的方法不是线程安全的，或者其内部状态可能在迭代过程中被其他协程修改（例如连接被移除），则可能存在风险。虽然 `connManager` (Zinx 的 `IConnManager`) 通常是线程安全的，但最佳实践是明确操作的原子性或在适当的锁保护下进行。
      - **操作**：考虑在整个迭代期间保持读锁，或者如果 `connManager.Get` 是耗时操作且确实安全，则当前方式可以接受，但需有文档说明。更稳妥的方式是复制 `deviceIdToConnMap` 后，在循环内对每个 `connID` 获取连接时，如果 `connManager` 的操作需要保护，则应有其自身的并发控制。鉴于 `IConnManager` 是 Zinx 核心组件，其 `Get` 方法通常是并发安全的。当前实现复制了 `deviceIdToConnMap` 并在复制后释放了 `mapMutex` 的读锁，这允许回调函数（如果它们需要写锁的话）不会导致死锁。此设计倾向于允许回调的灵活性。我们将维持当前设计，但强调 `connManager.Get` 必须是并发安全的。
      - **预期**：逻辑清晰，并发安全，文档明确。
    - **日志审查**：确保日志信息在各个关键路径上都足够、准确，便于问题排查。
    - **编译与测试**：
      - **操作**：确保代码能够成功编译。如果有单元测试，确保通过；如果没有，考虑为核心功能（如绑定、解绑、查询）添加单元测试。
      - **预期**：代码可编译，核心功能经过测试。

**第二阶段：适配上层和关联模块**

2.  **调整 `DeviceRegisterHandler` (`internal/infrastructure/zinx_server/handlers/device_register_handler.go`)**：

    - **操作**：
      - 确保从 `PhysicalID` 生成唯一的 `DeviceID`。
      - 正确调用 `TCPMonitor.BindDeviceIdToConnection` 将 `DeviceID` 与连接绑定。
      - 正确调用 `SessionManager.CreateSession` 创建设备会话，使用 `DeviceID`。
      - 根据业务逻辑，在连接上设置必要的属性（例如 `ICCID`）。`TCPMonitor` 不再负责设置这些业务属性。
    - **预期**：设备注册时，能为每个 `PhysicalID` 创建独立的会话，并正确关联到共享连接。

3.  **验证 `SessionManager` 和 `DeviceGroupManager` (`pkg/monitor/session_manager.go`, `pkg/monitor/device_group.go`)**：

    - **操作**：
      - 审查 `CreateSession`、`HandleDeviceDisconnect` 等方法，确保它们与新的 `TCPMonitor` 行为兼容，特别是 `DeviceID` 的使用。
      - 验证设备组管理逻辑是否仍能正确工作。
    - **预期**：会话和设备组管理功能正常。

4.  **验证发送逻辑和应用层查找 (`pkg/protocol/sender.go` 及相关应用层代码)**：
    - **操作**：
      - 确保下发指令时，应用层能够通过 `DeviceID` 找到对应的 `DeviceSession`。
      - `sender.go` 或类似模块在发送数据时，应通过 `DeviceID` 从 `TCPMonitor.GetConnectionByDeviceId` 获取到正确的共享连接进行发送。
      - 验证应用层是否能通过 `ICCID` 查找到所有关联的 `DeviceSession`（可能需要 `SessionManager` 提供相应接口，例如 `GetSessionsByICCID(iccid string) []*DeviceSession`，内部再按 `DeviceID` 区分）。
    - **预期**：能够向特定子设备发送指令，并能从应用层正确查找到各子设备的会话。

**第三阶段：集成测试与问题解决**

5.  **集成测试**：

    - **操作**：模拟主从设备（多个 `PhysicalID`）通过同一 TCP 连接注册和通信的场景。
      - 测试设备上线、下线。
      - 测试向特定子设备发送指令并接收响应。
      - 测试通过 `ICCID` 查询所有子设备状态。
      - 测试并发注册和断开。
    - **预期**：系统在多设备单连接场景下稳定运行，满足需求。

6.  **问题修复与迭代**：根据测试结果修复发现的 BUG。

**第四阶段：代码审查与文档更新**

7.  **代码审查**：对所有修改过的代码进行最终审查。
8.  **文档更新**：更新相关设计文档、注释，清晰说明多设备单连接的处理机制。
